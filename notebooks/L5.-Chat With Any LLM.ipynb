{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import IPython.display\n",
    "from PIL import Image\n",
    "import base64 \n",
    "import requests \n",
    "requests.adapters.DEFAULT_TIMEOUT = 60\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "hf_api_key = os.environ['HF_API_KEY']\n",
    "import gradio as gr\n",
    "import requests\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building An App To Chat With Any LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll be using an [Inference Endpoint](https://huggingface.co/inference-endpoints) for `falcon-40b-instruct` , the best ranking open source LLM on the [ðŸ¤— Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://pgwpq7ijn5o78468.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "headers = {\n",
    "\t\"Accept\" : \"application/json\",\n",
    "\t\"Authorization\": \"Bearer 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\",\n",
    "\t\"Content-Type\": \"application/json\" \n",
    "}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "\t\"parameters\": {}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '\"problem\"?'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '\\nMath has been both invented and discovered. It is a human invention in the sense that it is a system of rules and concepts that we have created to help us understand the world around us. However, it is also a discovery in the sense that it is a fundamental aspect of the universe that we have uncovered through observation and experimentation.'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://pgwpq7ijn5o78468.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer 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\",\n",
    "    \"Content-Type\": \"application/json\" \n",
    "}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Specify the model you want to use\n",
    "model_name = \"tiiuae/falcon-40b-instruct\"  # Replace with the model of your choice\n",
    "\n",
    "output = query({\n",
    "    \"inputs\": \"Has math been invented or discovered?\",\n",
    "    \"parameters\": {\"model\": model_name, \"max_new_tokens\": 256}\n",
    "})\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: [{'generated_text': '\\nArtificial intelligence (AI) is the simulation of human intelligence processes by computer systems. These processes include learning (the acquisition of information and rules for using the information), reasoning (using rules to reach approximate or definite conclusions) and self-correction.'}]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "\n",
    "# Define the query function for interacting with the Hugging Face API\n",
    "API_URL = \"https://pgwpq7ijn5o78468.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer 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\",\n",
    "    \"Content-Type\": \"application/json\" \n",
    "}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    response_json = response.json()\n",
    "    print(\"API Response:\", response_json)  # Print the API response for debugging\n",
    "    return response_json\n",
    "\n",
    "# Define the generate function for the Gradio interface\n",
    "def generate(input_text, slider_value):\n",
    "    payload = {\n",
    "        \"inputs\": input_text,\n",
    "        \"parameters\": {\"max_new_tokens\": slider_value}\n",
    "    }\n",
    "    output = query(payload)\n",
    "    if output and isinstance(output, list) and 'generated_text' in output[0]:\n",
    "        return output[0]['generated_text']\n",
    "    else:\n",
    "        # If the structure is different, return a formatted error message\n",
    "        return f\"Error in generation: {output}\"\n",
    "\n",
    "# Set up and launch the Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=generate,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Prompt\", placeholder=\"Type your prompt here...\"), \n",
    "        gr.Slider(label=\"Max new tokens\", value=20, maximum=1024, minimum=1)\n",
    "    ], \n",
    "    outputs=gr.Textbox(label=\"Completion\")\n",
    ")\n",
    "\n",
    "# Close any existing Gradio interfaces and launch the new one\n",
    "gr.close_all()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `gr.Chatbot()`\n",
    "\n",
    "- `gr.Chatbot()` allows you to save the chat history (between the user and the LLM) as well as display the dialogue in the app.\n",
    "- Define your `fn` to take in a `gr.Chatbot()` object.  \n",
    "  - Within your defined `fn` function, append a tuple (or a list) containing the user message and the LLM's response:\n",
    "`chatbot_object.append( (user_message, llm_message) )`\n",
    "\n",
    "- Include the chatbot object in both the inputs and the outputs of the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def respond(message, chat_history):\n",
    "        #No LLM here, just respond with a random pre-made message\n",
    "        bot_message = random.choice([\"Tell me more about it\", \n",
    "                                     \"Cool, but I'm not interested\", \n",
    "                                     \"Hmmmm, ok then\"]) \n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format the prompt with the chat history\n",
    "\n",
    "- You can iterate through the chatbot object with a for loop.\n",
    "- Each item is a tuple containing the user message and the LLM's message.\n",
    "\n",
    "```Python\n",
    "for turn in chat_history:\n",
    "    user_msg, bot_msg = turn\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Running on local URL:  http://127.0.0.1:7865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: [{'generated_text': \" I'm sorry, but as an AI language model, I don't have a personal belief system or a definitive answer to that question. However, many philosophers and religious leaders have proposed different theories and beliefs about the meaning of life.\"}]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "\n",
    "# Define the query function for interacting with the Hugging Face API\n",
    "API_URL = \"https://pgwpq7ijn5o78468.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer 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\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    response_json = response.json()\n",
    "    print(\"API Response:\", response_json)  # Print the API response for debugging\n",
    "    return response_json\n",
    "\n",
    "# Define the generate function for the Gradio interface\n",
    "def generate(input_text, max_new_tokens=1024, stop_sequences=None):\n",
    "    payload = {\n",
    "        \"inputs\": input_text,\n",
    "        \"parameters\": {\"max_new_tokens\": max_new_tokens}\n",
    "    }\n",
    "    if stop_sequences:\n",
    "        payload[\"parameters\"][\"stop_sequences\"] = stop_sequences\n",
    "\n",
    "    output = query(payload)\n",
    "    if output and isinstance(output, list) and 'generated_text' in output[0]:\n",
    "        return output[0]['generated_text']\n",
    "    else:\n",
    "        return \"Error in generation\"\n",
    "\n",
    "def format_chat_prompt(message, chat_history):\n",
    "    prompt = \"\"\n",
    "    for turn in chat_history:\n",
    "        user_message, bot_message = turn\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt\n",
    "\n",
    "def respond(message, chat_history):\n",
    "    formatted_prompt = format_chat_prompt(message, chat_history)\n",
    "    bot_message = generate(formatted_prompt, stop_sequences=[\"\\nUser:\", \"\"])\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chat_history = gr.State([])\n",
    "    chatbot = gr.Chatbot(height=240)\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chat_history], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chat_history], outputs=[msg, chatbot])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Other Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_prompt(message, chat_history, instruction):\n",
    "    prompt = f\"System:{instruction}\"\n",
    "    for turn in chat_history:\n",
    "        user_message, bot_message = turn\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "- If your LLM can provide its tokens one at a time in a stream, you can accumulate those tokens in the chatbot object.\n",
    "- The `for` loop in the following function goes through all the tokens that are in the stream and appends them to the most recent conversational turn in the chatbot's message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(message, chat_history, instruction, temperature=0.7):\n",
    "    # Format the prompt with chat history and new message\n",
    "    prompt = format_chat_prompt(message, chat_history, instruction)\n",
    "    \n",
    "    # Update chat history with the new message\n",
    "    chat_history.append([message, \"\"])\n",
    "\n",
    "    # Create the payload for the API request\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"temperature\": temperature,\n",
    "            \"stop_sequences\": [\"\\nUser:\", \"\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Use the query function to get the response from the API\n",
    "    output = query(payload)\n",
    "\n",
    "    # Extract and return the generated text\n",
    "    if output and isinstance(output, dict) and 'generated_text' in output:\n",
    "        # Update chat history with the response\n",
    "        chat_history[-1][1] = output['generated_text']\n",
    "        return output['generated_text']\n",
    "    else:\n",
    "        return \"Error in generation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Closing server running on port: 7863\n",
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    with gr.Accordion(label=\"Advanced options\",open=False):\n",
    "        system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\n",
    "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.queue().launch()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, in the cell above, you have used `demo.queue().launch()` instead of `demo.launch()`. \"queue\" helps you to boost up the performance for your demo. You can read [setting up a demo for maximum performance](https://www.gradio.app/guides/setting-up-a-demo-for-maximum-performance) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
